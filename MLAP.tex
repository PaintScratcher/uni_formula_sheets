\documentclass{article}
\begin{document}
\begin{center}
\Large\textbf{MLAP Formulas}\
\large\textit{Adam Taylor}
\newline
\end{center}
\section{Lecture 1:}
De Morgans Law:\\ $\mathbf{not(A and B) == not(A) or not(B)}$\\
$\mathbf{not(A or B) == (not A) and (not B)}$\\\\
Axioms of Probability:\\ $\mathbf{1: 0 \leq p(A) \leq 1}$\\
$\mathbf{2: p(S) = 1}$\\
$\mathbf{3: p(A \cup B ) = p(A) + p(B)}$\\\\
Conditional Probability: $\mathbf {p(A|B) = \frac{p(A\cap B)}{p(B)}}$\\\\
Estimated by using: $\mathbf{p(A|B) \approx \frac{c(A,B)}{c(B)}}$\\\\
A is independent iff $\mathbf{p(A|B) = p(A)}$\\\\
Therefore A and B are independent iff $\mathbf{p(A,B) = p(A)p(B)}$\\\\
Bayes Theorem: $\mathbf{p(A|B) = \frac{p(B|A)p(A)}{p(B)}}$
\section{Lecture 2:}
Binomial Distribution: $\mathbf{p(X = k|N, \theta) = \frac{N!}{k!(N-K)!}\theta^k(1-\theta)^{N-k}}$\\\\
Expectation of a discrete random variable: $\mathbf{E[X] = \sum\limits_{i}^{} x_ip(X=x_i)}$\\\\
Variance of a random variable: $\mathbf{Var(X) = E[(X-E[X])^2]}$\\\\
Covariance: $\mathbf{Cov(X,Y) = E[(X-E[X])(Y-E[Y])]}$
\end{document}


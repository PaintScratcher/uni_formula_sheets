\documentclass{article}
\begin{document}
\begin{center}
\Large\textbf{ICOT Spring Term Formulas}\
\large\textit{Adam Taylor}
\newline
\end{center}
Expected length of a compressed code: \\
$\mathbf{H(X) \leq L(C,X)\geq H(X)+1}$\\\\
Cost of a codeword in a codeword supermarket: $2^{-l}$ where l is the length \\\\
Kraft inequality: $\mathbf{\sum\limits_{k=1}^{d}2^{{-l_k}}\leq1}$ where d is the number of code words and l is the length of codewords\\\\
Information content: $\mathbf{log_2\frac{1}{p_k}}$\\\\
Shannon code length: $\mathbf {l_k = \lceil log_2\frac{1}{p_k}\rceil }$\\\\
Rate of code: $\mathbf{R=\frac{k}{n}}$ where k is encoded information and n is the number of bits in the codewords\\\\
Amount of bit flips that can be corrected using repetition code: $\mathbf{t = \frac{(n-1)}{2}}$\\\\
Error correction distance: $\mathbf{t=\lfloor\frac{d-1}{2}\rfloor}$\\\\
Average bits per symbol: $\mathbf{\sum p(x)l(x)}$\\\\
Differential entropy (Used for finding compression overhead for wrong codes):$\mathbf{D(p||q)=\sum p(x)log_2\frac{p(x)}{q(x)}}$\\\\
% Probability of having an error string with k bit-flips: $\mathbf{p^k =(1-p)^{n-k}}$\\\\
%Number of error strings with k bit-flips: $\mathbf{{n\choose b} = \frac{n!}{k!(n-k)!}}$\\\\
Probability of code error (having more than t bit-flips):\\ $\mathbf{P_{err}(n,p) = \sum\limits_{k=t+1}^{n}{n\choose k}p^k(1-p)^{n-k}}$\\\\

\end{document}